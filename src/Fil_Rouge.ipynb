{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import urllib\n",
    "import pdfx\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement des PDF sur Arkiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre : MaxViT: Multi-Axis Vision Transformer\n",
      "Auteurs : [arxiv.Result.Author('Zhengzhong Tu'), arxiv.Result.Author('Hossein Talebi'), arxiv.Result.Author('Han Zhang'), arxiv.Result.Author('Feng Yang'), arxiv.Result.Author('Peyman Milanfar'), arxiv.Result.Author('Alan Bovik'), arxiv.Result.Author('Yinxiao Li')]\n",
      "Lien : http://arxiv.org/pdf/2204.01697v1\n"
     ]
    }
   ],
   "source": [
    "#Utilisation de la librairie ArXiv pou récupérer le lien des pdf\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"computer science &  ai\",\n",
    "    # id_list=[\"1605.08386v1\"]\n",
    "    max_results = 1,\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "for result in search.results():\n",
    "    print (\"Titre :\", result.title)\n",
    "    print (\"Auteurs :\", result.authors)\n",
    "    print (\"Lien :\", result.pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zhengzhong_Tu', 'Hossein_Talebi', 'Han_Zhang', 'Feng_Yang', 'Peyman_Milanfar', 'Alan_Bovik', 'Yinxiao_Li']\n"
     ]
    }
   ],
   "source": [
    "# On met les auteurs sur forme de liste pour faciliter le traitement\n",
    "\n",
    "for result in search.results():\n",
    "    author = result.authors\n",
    "    author_list = [re.sub(\"[^A-Za-z0-9]\",\"_\",str(i)) for i in author]\n",
    "\n",
    "print (author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pdf & store it\n",
    "\n",
    "response = urllib.request.urlopen(result.pdf_url)    \n",
    "file = open(\"../data/Test\" + \".pdf\", 'wb')\n",
    "file.write(response.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leture du PDF\n",
    "\n",
    "pdf = pdfx.PDFx(\"../data/Test.pdf\")\n",
    "metadata = pdf.get_metadata()\n",
    "references_list = pdf.get_references()\n",
    "references_dict = pdf.get_references_as_dict()\n",
    "text = pdf.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationDate': 'D:20220405011353Z', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20220405011353Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Trapped': 'False', 'Pages': 28}\n",
      "2107.00641\n"
     ]
    }
   ],
   "source": [
    "print (metadata)\n",
    "#print (references_list)\n",
    "print (references_dict['url'][0])\n",
    "#print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du champ de recherche à partir du mot \"Références\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_references(mypdftext): \n",
    "    keyword1 = 'References'\n",
    "    keyword2 = 'REFERENCES'\n",
    "    keyword3 = 'R EFERENCES'\n",
    "    keyword4 = 'Reference'\n",
    "    keyword5='[1]' \n",
    "\n",
    "    if keyword1 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
    "    elif keyword2 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
    "    elif keyword3 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
    "    elif keyword4 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
    "    elif keyword5 in mypdftext :\n",
    "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
    "    else:\n",
    "        after_keyword = mypdftext[:10000]\n",
    "    return after_keyword\n",
    "\n",
    "#All references in a variable\n",
    "\n",
    "references=after_references(text)\n",
    "# print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche des entités nommées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zhou\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "\n",
    "def extract(text:str) :\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text.strip())\n",
    "    named_entities = []\n",
    "    \n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        text = text.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"PERSON\"]:\n",
    "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
    "        named_entities = list(dict.fromkeys(named_entities))\n",
    "    return named_entities\n",
    "\n",
    "print (extract(references)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de l'ontologie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owlready2.JAVA_EXE='/usr/bin/java'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto = get_ontology('http://filrouge.org/') # nouvelle ontologie, on donne son IRI\n",
    "\n",
    "with onto:\n",
    "    # Define an Author\n",
    "    class Author(Thing):  # nouvelle classe sous-classe de Thing\n",
    "        pass\n",
    "    \n",
    "    # Define a Reference\n",
    "    class Reference(Thing): pass\n",
    "    \n",
    "    # An Author get ideas from theirs references\n",
    "    class getIdeasFrom(Author >> Reference): pass\n",
    "        \n",
    "    # An Author is influenced by the references of the references\n",
    "    class isInfluencedBy(Author >> Reference): pass\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "102e1a2b2b206be7da3552768dc0a5198c57d5a20116016c35db6323ca3d0506"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
