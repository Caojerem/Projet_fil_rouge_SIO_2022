{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xXIVklbte3rg"
      },
      "outputs": [],
      "source": [
        "# !pip install arxiv\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# !pip install pdfx\n",
        "# !pip install owlready2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PYI34bZse2z_"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import urllib\n",
        "import pdfx\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuR7VNOIe20D"
      },
      "source": [
        "# Téléchargement des PDF sur Arkiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfJfGTYe20E"
      },
      "source": [
        "### Utilisation de la librairie ArXiv pou récupérer le lien des pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDnaA3Kae20E",
        "outputId": "07ef3b4a-ffb7-4efa-9367-1a95b8911ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titre : Temporal Alignment Networks for Long-term Video\n",
            "Auteurs : [arxiv.Result.Author('Tengda Han'), arxiv.Result.Author('Weidi Xie'), arxiv.Result.Author('Andrew Zisserman')]\n",
            "Lien : http://arxiv.org/pdf/2204.02968v1\n",
            "Titre : Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation\n",
            "Auteurs : [arxiv.Result.Author('Sravya Popuri'), arxiv.Result.Author('Peng-Jen Chen'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Juan Pino'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Jiatao Gu'), arxiv.Result.Author('Wei-Ning Hsu'), arxiv.Result.Author('Ann Lee')]\n",
            "Lien : http://arxiv.org/pdf/2204.02967v1\n"
          ]
        }
      ],
      "source": [
        "search = arxiv.Search(\n",
        "    query = \"computer science &  ai\",\n",
        "    # id_list=[\"1605.08386v1\"]\n",
        "    max_results = 2,\n",
        "    sort_by = arxiv.SortCriterion.SubmittedDate\n",
        ")\n",
        "\n",
        "for result in search.results():\n",
        "    print (\"Titre :\", result.title)\n",
        "    print (\"Auteurs :\", result.authors)\n",
        "    print (\"Lien :\", result.pdf_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SANmaBseuRAO",
        "outputId": "6cf7cc0a-c728-4c44-b1b6-6b067c124280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://arxiv.org/pdf/2204.02967v1\n"
          ]
        }
      ],
      "source": [
        "print(result.pdf_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rwXOKZZe20G",
        "outputId": "4902aada-91cb-40fa-c0a2-6caa8bbd174f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Sravya_Popuri', 'Peng_Jen_Chen', 'Changhan_Wang', 'Juan_Pino', 'Yossi_Adi', 'Jiatao_Gu', 'Wei_Ning_Hsu', 'Ann_Lee']\n"
          ]
        }
      ],
      "source": [
        "# On met les auteurs sur forme de liste pour faciliter le traitement\n",
        "\n",
        "for result in search.results():\n",
        "    author = result.authors\n",
        "    author_list = [re.sub(\"[^A-Za-z0-9]\",\"_\",str(i)) for i in author]\n",
        "\n",
        "print (author_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f8pIgls4e20G"
      },
      "outputs": [],
      "source": [
        "# Download the pdf & store it\n",
        "\n",
        "response = urllib.request.urlopen(result.pdf_url)    \n",
        "file = open(\"Test\" + \".pdf\", 'wb')\n",
        "file.write(response.read())\n",
        "file.close()\n",
        "\n",
        "# Leture du PDF\n",
        "\n",
        "pdf = pdfx.PDFx(\"Test.pdf\")\n",
        "metadata = pdf.get_metadata()\n",
        "references_dict = pdf.get_references_as_dict()\n",
        "text = pdf.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdfkDADEe20H",
        "outputId": "ef1af3cc-1a9f-4f9e-8a45-c7f5ed89757e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'CreationDate': 'D:20220407004439Z', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20220407004439Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Trapped': 'False', 'Pages': 7}\n",
            "['2107.05604', '2112.08352', 'https://github', '2010.11745', '2105.01051', 'https://huggingface.co/facebook/tts_', 'https://huggingface.co/facebook/tts_transformer-en-ljspeech', 'facebookresearch.github.io/speech_translation/', 'https://huggingface.co/facebook/tts_transformer-es-css10', 'https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-spanish', '2104.06678', 'https://github.com/pytorch/fairseq/blob/main/examples/speech_to_speech/docs/textless_s2st_real_data.md', 'https://huggingface', '2111.07402', '2007.10310', '2012.03411', 'github.com/pytorch/fairseq/blob/', 'https://huggingface.co/jonatasgrosman/', '2104.00355', '2109.03264', '2104.02133', '2102.01192', 'https://github.com/pytorch/fairseq/blob/main/examples/speech_to_speech/docs/enhanced_direct_s2st_discrete_units.md', '1912.06670', 'https://huggingface.co/facebook/', '2101.00390', '2110.07205', 'units.md', '2110.13900', '2107.08661', '2106.07447', 'https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self', '2006.07926', 'https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html']\n"
          ]
        }
      ],
      "source": [
        "print (metadata)\n",
        "# print (references_list)\n",
        "print (references_dict['url'])\n",
        "#print (text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9kIsVFAe20H"
      },
      "source": [
        "### Téléchargement d'une référence afin d'avoir des liens pertinents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIHOZUpoe20I",
        "outputId": "adc8993b-d850-42c6-9634-0b1e050865f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n",
            "Titre : Direct speech-to-speech translation with discrete units\n",
            "Auteurs : [arxiv.Result.Author('Ann Lee'), arxiv.Result.Author('Peng-Jen Chen'), arxiv.Result.Author('Changhan Wang'), arxiv.Result.Author('Jiatao Gu'), arxiv.Result.Author('Sravya Popuri'), arxiv.Result.Author('Xutai Ma'), arxiv.Result.Author('Adam Polyak'), arxiv.Result.Author('Yossi Adi'), arxiv.Result.Author('Qing He'), arxiv.Result.Author('Yun Tang'), arxiv.Result.Author('Juan Pino'), arxiv.Result.Author('Wei-Ning Hsu')]\n",
            "Lien : http://arxiv.org/pdf/2107.05604v2\n"
          ]
        }
      ],
      "source": [
        "print(len(references_dict['url']))\n",
        "for i in range(len(references_dict['url'])):\n",
        "    try :\n",
        "        search_ref = arxiv.Search(id_list=[references_dict['url'][i]])\n",
        "        for result in search_ref.results():\n",
        "            print (\"Titre :\", result.title)\n",
        "            print (\"Auteurs :\", result.authors)\n",
        "            print (\"Lien :\", result.pdf_url)\n",
        "        break\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K88ktrBTe20I",
        "outputId": "413fe3ed-b140-490f-8783-e0cbbebee9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ann_Lee', 'Peng_Jen_Chen', 'Changhan_Wang', 'Jiatao_Gu', 'Sravya_Popuri', 'Xutai_Ma', 'Adam_Polyak', 'Yossi_Adi', 'Qing_He', 'Yun_Tang', 'Juan_Pino', 'Wei_Ning_Hsu']\n"
          ]
        }
      ],
      "source": [
        "for result in search_ref.results():\n",
        "    author = result.authors\n",
        "    author_ref_list = [re.sub(\"[^A-Za-z0-9]\",\"_\",str(i)) for i in author]\n",
        "\n",
        "print (author_ref_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TxgCxt56e20J"
      },
      "outputs": [],
      "source": [
        "# Download the pdf & store it\n",
        "\n",
        "response = urllib.request.urlopen(result.pdf_url)    \n",
        "file = open(\"Ref\" + \".pdf\", 'wb')\n",
        "file.write(response.read())\n",
        "file.close()\n",
        "\n",
        "# Leture du PDF\n",
        "\n",
        "pdf = pdfx.PDFx(\"Ref.pdf\")\n",
        "metadata_ref = pdf.get_metadata()\n",
        "references_ref_dict = pdf.get_references_as_dict()\n",
        "text_ref = pdf.get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Kznli-e20J"
      },
      "source": [
        "### Optimisation du champ de recherche à partir du mot \"Références\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ia8EMcOke20J"
      },
      "outputs": [],
      "source": [
        "def after_references(mypdftext): \n",
        "    keyword1 = 'References'\n",
        "    keyword2 = 'REFERENCES'\n",
        "    keyword3 = 'R EFERENCES'\n",
        "    keyword4 = 'Reference'\n",
        "    keyword5='[1]' \n",
        "\n",
        "    if keyword1 in mypdftext :\n",
        "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword1)\n",
        "    elif keyword2 in mypdftext :\n",
        "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword2)\n",
        "    elif keyword3 in mypdftext :\n",
        "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword3)\n",
        "    elif keyword4 in mypdftext :\n",
        "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword4)\n",
        "    elif keyword5 in mypdftext :\n",
        "            before_keyword, keyword, after_keyword = mypdftext.partition(keyword5)\n",
        "    else:\n",
        "        after_keyword = mypdftext[:10000]\n",
        "    return after_keyword\n",
        "\n",
        "#All references in a variable\n",
        "\n",
        "references = after_references(text)\n",
        "references_ref = after_references(text_ref)\n",
        "\n",
        "# print(references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIKoyvpe20K"
      },
      "source": [
        "# Recherche des entités nommées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Commande à executer la première fois car le package ne s'installe pas bien avec le fichier \"requirement\"\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3QFlkf6e20K",
        "outputId": "aa9ea911-dde6-4257-b15b-4d916baca2b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.fr.examples import sentences \n",
        "\n",
        "def extract(text:str) :\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text.strip())\n",
        "    named_entities = []\n",
        "    \n",
        "    for i in doc.ents:\n",
        "        entry = str(i.lemma_).lower()\n",
        "        text = text.replace(str(i).lower(), \"\")\n",
        "        if i.label_ in [\"PERSON\"]:\n",
        "            named_entities.append(entry.title().replace(\" \", \"_\").replace(\"\\n\",\"_\"))\n",
        "        named_entities = list(dict.fromkeys(named_entities))\n",
        "    return named_entities\n",
        "\n",
        "references_list = extract(references)\n",
        "references_ref_list = extract(references_ref)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_gjICye20L"
      },
      "source": [
        "### Traitement des charactères non reconnus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ACUw_Tdme20L"
      },
      "outputs": [],
      "source": [
        "from xml.sax.saxutils import escape\n",
        "import sys\n",
        "\n",
        "def invalid_xml_remove(char):\n",
        "    \"\"\"Tracks illegal unicode characters\"\"\"\n",
        "    #http://stackoverflow.com/questions/1707890\n",
        "    # /fast-way-to-filter-illegal-xml-unicode-chars-in-python\n",
        "    illegal_unichrs = [ (0x00, 0x08), (0x0B, 0x1F), (0x7F, 0x84), (0x86, 0x9F),\n",
        "                    (0xD800, 0xDFFF), (0xFDD0, 0xFDDF), (0xFFFE, 0xFFFF),\n",
        "                    (0x1FFFE, 0x1FFFF), (0x2FFFE, 0x2FFFF), (0x3FFFE, 0x3FFFF),\n",
        "                    (0x4FFFE, 0x4FFFF), (0x5FFFE, 0x5FFFF), (0x6FFFE, 0x6FFFF),\n",
        "                    (0x7FFFE, 0x7FFFF), (0x8FFFE, 0x8FFFF), (0x9FFFE, 0x9FFFF),\n",
        "                    (0xAFFFE, 0xAFFFF), (0xBFFFE, 0xBFFFF), (0xCFFFE, 0xCFFFF),\n",
        "                    (0xDFFFE, 0xDFFFF), (0xEFFFE, 0xEFFFF), (0xFFFFE, 0xFFFFF),\n",
        "                    (0x10FFFE, 0x10FFFF) ]\n",
        "\n",
        "    illegal_ranges = [f\"{chr(low)}-{chr(high)}\"\n",
        "                  for (low, high) in illegal_unichrs\n",
        "                  if low < sys.maxunicode]\n",
        "\n",
        "    illegal_xml_re = re.compile(f'[{\"\".join(illegal_ranges)}]')\n",
        "    if illegal_xml_re.search(char) is not None:\n",
        "        #Replace with space\n",
        "        return ''\n",
        "    else:\n",
        "        return char\n",
        "\n",
        "def clean_char(char):\n",
        "    \"\"\"\n",
        "    Function for remove invalid XML characters from\n",
        "    incoming data.\n",
        "    \"\"\"\n",
        "    #Get rid of the ctrl characters first.\n",
        "    #http://stackoverflow.com/questions/1833873/python-regex-escape-characters\n",
        "    char = re.sub('\\x1b[^m]*m', '', char)\n",
        "    #Clean up invalid xml\n",
        "    char = invalid_xml_remove(char)\n",
        "    replacements = [\n",
        "        ('\\u201c', '\\\"'),\n",
        "        ('\\u0141', '\\\"'),\n",
        "        ('\\u201d', '\\\"'),\n",
        "        (\"\\u001B\", ''), #http://www.fileformat.info/info/unicode/char/1b/index.htm\n",
        "        (\"\\u0019\", ''), #http://www.fileformat.info/info/unicode/char/19/index.htm\n",
        "        (\"\\u0016\", ''), #http://www.fileformat.info/info/unicode/char/16/index.htm\n",
        "        (\"\\u001C\", ''), #http://www.fileformat.info/info/unicode/char/1c/index.htm\n",
        "        (\"\\u0003\", ''), #http://www.utf8-chartable.de/unicode-utf8-table.pl?utf8=0x\n",
        "        (\"\\u000C\", ''),\n",
        "        (\"\\u03b1\", ''),\n",
        "        (\"u\\u039C\", ''),\n",
        "        (\"\\u03C3\", ''),\n",
        "        (\"\\u0141\", ''),\n",
        "        (\"\\u0308\", ''),\n",
        "        (\"\\u2032\", ''),\n",
        "        (\"\\u03b8\", '')\n",
        "\n",
        "    \n",
        "    ]\n",
        "    for rep, new_char in replacements:\n",
        "        if char == rep:\n",
        "            return new_char\n",
        "    return char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "917dvgNpe20M"
      },
      "outputs": [],
      "source": [
        "def clean_text(text: str):\n",
        "        \"\"\"Escape the illegal characters for an ontology property\"\"\"\n",
        "        if text is None:\n",
        "            return None\n",
        "        # function to escape XML character data\n",
        "        text = escape(text)\n",
        "        text = text.replace('\\n', '')\n",
        "        text = text.replace('\\r', '')\n",
        "        text = text.replace('\\f', '')\n",
        "        text = text.replace('\\b', '')\n",
        "        text = text.replace('\"', '')\n",
        "        text = text.replace('[', '')\n",
        "        text = text.replace(']', '')\n",
        "        text = text.replace('{', '')\n",
        "        text = text.replace('}', '')\n",
        "        text = text.replace('#', '')\n",
        "        text = text.replace('|', '')\n",
        "        text = text.replace(' ', '_')\n",
        "        text = text.replace('σ', '')\n",
        "        text = text.replace('ℓ', '')\n",
        "        text = text.replace('Σ', '')\n",
        "        text = text.replace('ı', '')\n",
        "        text = text.replace('ē', '')\n",
        "        text = text.replace('μ', '')\n",
        "        text = text.replace('Μ', '')\n",
        "        text = text.replace('Ł', '')\n",
        "        text = text.replace('ł', '')\n",
        "        text = text.replace('θ', '')\n",
        "        text = text.replace('φ', '')\n",
        "        text = text.replace('Φ', '')\n",
        "        text = text.replace('̈', '')\n",
        "        text = text.replace('́', '')\n",
        "        text = text.replace('′', '')\n",
        "        text = text.replace('∈', '')\n",
        "        text = text.replace('ć', '')\n",
        "        text = text.replace('ź', '')\n",
        "        text = text.replace('', 'a')\n",
        "        text = text.replace('⊕','')\n",
        "        text = text.replace('', 'b')\n",
        "        text = text.replace('', 'c')\n",
        "        text = clean_char(text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "29C4X_M1e20M"
      },
      "outputs": [],
      "source": [
        "for i in range(len(author_list)):\n",
        "    author_list[i] = clean_text(author_list[i])\n",
        "\n",
        "for i in range(len(references_list)):\n",
        "    references_list[i] = clean_text(references_list[i])\n",
        "\n",
        "for i in range(len(author_ref_list)):\n",
        "    author_ref_list[i] = clean_text(author_ref_list[i])\n",
        "\n",
        "for i in range(len(references_ref_list)):\n",
        "    references_ref_list[i] = clean_text(references_ref_list[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnrBTsVae20N"
      },
      "source": [
        "# Création de l'ontologie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ikchEi54e20N"
      },
      "outputs": [],
      "source": [
        "from owlready2 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EN9cfDpqe20N"
      },
      "outputs": [],
      "source": [
        "# owlready2.JAVA_EXE='C:/Program Files/Common Files/Oracle/Java/javapath/java.exe'\n",
        "owlready2.JAVA_EXE='/usr/bin/java'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8el9mAPe20N"
      },
      "source": [
        "### Création des classes et des propriétés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tD2TxjnBe20O"
      },
      "outputs": [],
      "source": [
        "onto = get_ontology('http://filrouge.org/onto.owl') # nouvelle ontologie, on donne son IRI\n",
        "\n",
        "with onto:\n",
        "    # Define an Person\n",
        "    class Person(Thing):  # nouvelle classe sous-classe de Thing\n",
        "        pass\n",
        "    \n",
        "    # An Person get ideas from another Person\n",
        "    class gets_ideas_from(Person >> Person): pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6WJHAAme20O"
      },
      "source": [
        "#### Ajout des entités du document dans l'ontologie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5ncIaOije20O"
      },
      "outputs": [],
      "source": [
        "with onto:\n",
        "    for i in range(2):\n",
        "        globals()[author_list[i]] = Person(name = author_list[i])\n",
        "        for j in range(2):\n",
        "            globals()[references_list[j]] = Person(name = references_list[j])\n",
        "            globals()[author_list[i]].gets_ideas_from.append(globals()[references_list[j]])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            globals()[author_ref_list[j]] = Person(name = author_ref_list[j])\n",
        "            globals()[author_list[i]].gets_ideas_from.append(globals()[author_ref_list[j]])\n",
        "    for i in range(2):\n",
        "            for j in range(2):\n",
        "                globals()[references_ref_list[j]] = Person(name = references_ref_list[j])\n",
        "                globals()[author_ref_list[i]].gets_ideas_from.append(globals()[references_ref_list[j]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "arGIXVi34Ahg"
      },
      "outputs": [],
      "source": [
        "with onto:\n",
        "    AllDifferent([globals()[author_list[0]],globals()[author_ref_list[0]],globals()[references_list[0]],globals()[references_ref_list[0]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFq_cK0Te20P"
      },
      "source": [
        "### Création de règles complexes pour l'ontologie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2wiU8PSIe20P"
      },
      "outputs": [],
      "source": [
        "with onto:\n",
        "    imp = Imp()\n",
        "    imp.set_as_rule(\"Person(?x), Person(?y), Person(?z), \\\n",
        "                     DifferentFrom(?x, ?y), DifferentFrom(?y, ?z), DifferentFrom(?x, ?z),  \\\n",
        "                     gets_ideas_from(?x, ?y), gets_ideas_from(?y, ?z) -> gets_ideas_from(?x, ?z)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi6eg__ux68E",
        "outputId": "3b3e7c8d-ab6f-4650-a1bb-b627d527a044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[onto.R._J._Weiss, onto.C._Zhang, onto.Ann_Lee, onto.Peng_Jen_Chen]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(globals()[author_list[0]].gets_ideas_from))\n",
        "globals()[author_list[0]].gets_ideas_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WULl3jEXydvu",
        "outputId": "ac654aa9-7400-4717-fc0d-cb6b83caa448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[onto.Pd_Aguero, onto.Jordi_Adell]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(globals()[author_ref_list[0]].gets_ideas_from))\n",
        "globals()[author_ref_list[0]].gets_ideas_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO1B80g30J-Y",
        "outputId": "d5da488f-d892-471c-d18c-2b06ba88646d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "onto.Sravya_Popuri\n",
            "onto.R._J._Weiss\n",
            "onto.C._Zhang\n",
            "onto.Peng_Jen_Chen\n",
            "onto.Ann_Lee\n",
            "onto.Pd_Aguero\n",
            "onto.Jordi_Adell\n"
          ]
        }
      ],
      "source": [
        "with onto:\n",
        "  print(len(Person.instances()))\n",
        "  for p in Person.instances(): \n",
        "    print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfnMLACUe20P",
        "outputId": "91451266-3949-40e4-abdd-6bd28daf8c65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "* Owlready2 * Running Pellet...\n",
            "    /usr/bin/java -Xmx2000M -cp /usr/local/lib/python3.7/dist-packages/owlready2/pellet/httpclient-4.2.3.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jena-core-2.10.0.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/aterm-java-1.6.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/commons-codec-1.6.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jena-iri-0.9.5.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/httpcore-4.2.2.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jena-tdb-0.10.0.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jena-arq-2.10.0.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/antlr-3.2.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/pellet-2.3.1.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/xercesImpl-2.10.0.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/slf4j-log4j12-1.6.4.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/xml-apis-1.4.01.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/log4j-1.2.16.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/slf4j-api-1.6.4.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jcl-over-slf4j-1.6.4.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/antlr-runtime-3.2.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/jgrapht-jdk1.5.jar:/usr/local/lib/python3.7/dist-packages/owlready2/pellet/owlapi-distribution-3.4.3-bin.jar pellet.Pellet realize --loader Jena --input-format N-Triples --infer-prop-values --infer-data-prop-values --ignore-imports /tmp/tmp1tdzlcar\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Owlready * Adding relation onto.Jordi_Adell gets_ideas_from onto.Ann_Lee\n",
            "* Owlready * Adding relation onto.Sravya_Popuri gets_ideas_from onto.Pd_Aguero\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "* Owlready2 * Pellet took 1.4212651252746582 seconds\n",
            "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
          ]
        }
      ],
      "source": [
        "sync_reasoner_pellet([onto], infer_property_values=True, infer_data_property_values=True, debug=True, keep_tmp_file=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiAQ0XpS92xE",
        "outputId": "8c03b9c7-431a-423d-dd37-24b497d3a41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[onto.R._J._Weiss,\n",
              " onto.C._Zhang,\n",
              " onto.Ann_Lee,\n",
              " onto.Peng_Jen_Chen,\n",
              " onto.Pd_Aguero]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(globals()[author_list[0]].gets_ideas_from))\n",
        "globals()[author_list[0]].gets_ideas_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IN4G8wzMe20P"
      },
      "outputs": [],
      "source": [
        "onto.save('onto.owl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pUFn9Q4R5cQ_"
      },
      "outputs": [],
      "source": [
        "onto.destroy('onto.owl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Fil_Rouge.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "102e1a2b2b206be7da3552768dc0a5198c57d5a20116016c35db6323ca3d0506"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
